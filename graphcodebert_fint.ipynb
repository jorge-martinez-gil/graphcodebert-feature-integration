{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8681f5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# This installs the Hugging Face transformers library with support for PyTorch.\n",
    "# The '-U' flag ensures the package is upgraded to the latest version if it's already installed.\n",
    "!pip install transformers[torch] -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df29fd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# PyTorch is used for building and training neural networks.\n",
    "# The Hugging Face transformers library is used for working with transformer models like RoBERTa.\n",
    "# We also import libraries for handling datasets, managing memory, and evaluating model performance.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, AutoTokenizer, AutoModel, Trainer, TrainingArguments, EarlyStoppingCallback\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import json\n",
    "import random\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "# Memory management\n",
    "import gc  # Python's garbage collector\n",
    "torch.cuda.empty_cache()  # Clear unused memory in the GPU\n",
    "gc.collect()  # Collect and free unused memory in the RAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb3a355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom model class\n",
    "# This class defines a custom neural network that extends a pre-trained RoBERTa model\n",
    "# with additional features and a classification head.\n",
    "\n",
    "class RobertaForSequenceClassificationWithOutput(nn.Module):\n",
    "    def __init__(self, num_labels=2, output_feature_dim=1):\n",
    "        super().__init__()\n",
    "        # Load the pre-trained RoBERTa model from the Hugging Face model hub.\n",
    "        self.num_labels = num_labels\n",
    "        self.roberta = AutoModel.from_pretrained('microsoft/graphcodebert-base')\n",
    "        \n",
    "        # Define a linear layer to process the additional input feature.\n",
    "        self.output_feature_layer = nn.Linear(output_feature_dim, self.roberta.config.hidden_size)\n",
    "        \n",
    "        # Define a classification head that combines the RoBERTa output with the additional feature.\n",
    "        self.classifier = nn.Linear(self.roberta.config.hidden_size + self.roberta.config.hidden_size, num_labels)\n",
    "        \n",
    "        # Define a dropout layer to prevent overfitting.\n",
    "        self.dropout = nn.Dropout(self.roberta.config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, labels=None, output_feature=None):\n",
    "        # Pass the input through the RoBERTa model to get the pooled output.\n",
    "        outputs = self.roberta(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.pooler_output\n",
    "\n",
    "        # Process the additional feature and concatenate it with the pooled output.\n",
    "        output_feature_processed = self.output_feature_layer(output_feature.unsqueeze(-1))\n",
    "        combined_features = torch.cat((pooled_output, output_feature_processed), dim=1)\n",
    "        combined_features = self.dropout(combined_features)\n",
    "\n",
    "        # Pass the combined features through the classifier to get the logits.\n",
    "        logits = self.classifier(combined_features)\n",
    "\n",
    "        # Compute the loss if labels are provided.\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "\n",
    "        # Return the output as a SequenceClassifierOutput object.\n",
    "        return SequenceClassifierOutput(loss=loss, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48ea0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset class\n",
    "# This class handles loading and tokenizing data for training and evaluation.\n",
    "\n",
    "class CodePairDataset(Dataset):\n",
    "    def __init__(self, file_path, tokenizer):\n",
    "        # Load the data from a JSON file.\n",
    "        with open(file_path, 'r') as file:\n",
    "            self.data = json.load(file)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve a single data item and tokenize it.\n",
    "        item = self.data[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text=item[\"code1\"], \n",
    "            text_pair=item[\"code2\"], \n",
    "            truncation=True, \n",
    "            padding=\"max_length\", \n",
    "            max_length=512, \n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        encoding = {key: val.squeeze(0) for key, val in encoding.items()}  # Remove batch dimension\n",
    "        \n",
    "        # Add the labels and additional feature to the encoding.\n",
    "        encoding['labels'] = torch.tensor(item[\"score\"], dtype=torch.long)\n",
    "        encoding['output_feature'] = torch.tensor(item[\"output\"], dtype=torch.float)\n",
    "        return encoding\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the total number of data items.\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b211873",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the evaluation metrics function\n",
    "# This function calculates the precision, recall, F1 score, and accuracy of the model's predictions.\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = predictions.argmax(-1)  # Get the predicted class by taking the argmax of the logits\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n",
    "    return {\n",
    "        'accuracy': accuracy_score(labels, predictions), \n",
    "        'f1': f1, \n",
    "        'precision': precision, \n",
    "        'recall': recall\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034b30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to execute the training and evaluation process\n",
    "# This function loads the data, prepares the model, and manages the training and evaluation workflow.\n",
    "\n",
    "def main():\n",
    "    # Load the tokenizer for the pre-trained model\n",
    "    tokenizer = AutoTokenizer.from_pretrained('microsoft/graphcodebert-base')\n",
    "    \n",
    "    # Specify the path to the dataset\n",
    "    dataset_path = '/content/drive/MyDrive/ensemble-codesim/karnalim/ensemble/data2.json'  # Replace with your dataset path\n",
    "    full_dataset = CodePairDataset(file_path=dataset_path, tokenizer=tokenizer)\n",
    "\n",
    "    # Split the dataset into training, validation, and test sets\n",
    "    train_size = int(0.8 * len(full_dataset))\n",
    "    test_val_size = len(full_dataset) - train_size\n",
    "    val_size = int(0.5 * test_val_size)  # Split the remaining data equally into validation and test sets\n",
    "    test_size = test_val_size - val_size\n",
    "\n",
    "    # Randomly split the dataset into train, validation, and test sets\n",
    "    train_dataset, remaining_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_val_size])\n",
    "    val_dataset, test_dataset = torch.utils.data.random_split(remaining_dataset, [val_size, test_size])\n",
    "\n",
    "    # Initialize the custom model\n",
    "    model = RobertaForSequenceClassificationWithOutput(num_labels=2, output_feature_dim=1)\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='/content/sample_data/results',  # Directory to save model checkpoints and outputs\n",
    "        num_train_epochs=3,  # Number of training epochs\n",
    "        per_device_train_batch_size=8,  # Batch size for training\n",
    "        warmup_steps=500,  # Number of warmup steps for learning rate scheduler\n",
    "        weight_decay=0.01,  # Strength of weight decay\n",
    "        logging_dir='./logs',  # Directory to save logs\n",
    "        evaluation_strategy=\"steps\",  # Evaluate the model at regular intervals (every `eval_steps`)\n",
    "        eval_steps=500,  # Number of steps between evaluations\n",
    "        save_strategy=\"steps\",  # Save the model at regular intervals (every `save_steps`)\n",
    "        save_steps=500,  # Number of steps between saving the model\n",
    "        load_best_model_at_end=True,  # Load the best model found during training at the end\n",
    "        metric_for_best_model=\"f1\",  # Metric to use to select the best model\n",
    "    )\n",
    "\n",
    "    # Initialize the Trainer class for training and evaluation\n",
    "    trainer = Trainer(\n",
    "        model=model,  # The model to be trained\n",
    "        args=training_args,  # Training arguments\n",
    "        train_dataset=train_dataset,  # Training dataset\n",
    "        eval_dataset=val_dataset,  # Evaluation dataset\n",
    "        compute_metrics=compute_metrics,  # Function to compute evaluation metrics\n",
    "        callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],  # Early stopping callback\n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.train()\n",
    "\n",
    "    # Evaluate the model on the validation set and print the results\n",
    "    val_results = trainer.evaluate(val_dataset)\n",
    "    print(f\"Validation Precision: {val_results['eval_precision']:.4f}\")\n",
    "    print(f\"Validation Recall: {val_results['eval_recall']:.4f}\")\n",
    "    print(f\"Validation F1 Score: {val_results['eval_f1']:.4f}\")\n",
    "\n",
    "    # Evaluate the model on the test set and print the results\n",
    "    test_results = trainer.evaluate(test_dataset)\n",
    "    print(f\"Test Precision: {test_results['eval_precision']:.4f}\")\n",
    "    print(f\"Test Recall: {test_results['eval_recall']:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74641607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the main function\n",
    "# This cell initiates the entire process by calling the main function.\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
